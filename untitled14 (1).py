# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pGVSTGv4OfUxV9wyjFID-3fKISB2eLv4

# **PROJECT TITLE  :  Youtube Adview Prediction**


---



**Objective :**

To build a machine learning regression to predict youtube adview count based
on other youtube metrics.

**Data :**

Use the below link to download the Data Set:
https://drive.google.com/file/d/1Dv-HF10AUUA03AO_cQvar462eXawk0iQ/view?usp=sharing





**Data Description :**

The file train.csv contains metrics and other details of about 15000 youtube
videos. The metrics include number of views, likes, dislikes, comments and
apart from that published date, duration and category are also included.
The train.csv file also contains the metric number of adviews which is our
target variable for prediction.


**Context :**

Youtube advertisers pay content creators based on adviews and clicks for the
goods and services being marketed. They want to estimate the adview based
on other metrics like comments, likes etc. The problem statement is therefore
to train various regression models and choose the best one to predict the
number of adviews. The data needs to be refined and cleaned before feeding
in the algorithms for better results.

**Attribute Information :**


'vidid' : Unique Identification ID for each video

'adview' : The number of adviews for each video

'views' : The number of unique views for each video

'likes' : The number of likes for each video

'dislikes' : The number of likes for each video

'comment' : The number of unique comments for each video

'published' : The data of uploading the video

'duration' : The duration of the video (in min. and seconds)

'category' : Category niche of each of the video

### **Step 1 : Import the datasets and libraries, check shape and datatype.**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns



# Load the dataset
data = pd.read_csv("/content/train.csv")

# Check the shape and datatype of the dataset
print(data.shape)
print(data.dtypes)
data

"""### **Step 2 : Visualise the dataset using plotting using heatmaps and plots.**

"""

# Visualization
# Individual Plots
plt.hist(data['category'])
plt.show()

plt.plot(data["adview"])
plt.show()

# Remove videos with adview greater than 2000000 as outlier data_train = data[data["adview"] <2000000]
#Solution : Youtube Adview Prediction 2

# Heatmap
import seaborn as sns
f, ax = plt.subplots(figsize=(10, 8))
corr = data.corr()
sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),
square=True, ax=ax,annot=True)
plt.show()

"""### **Step 3 : Clean the dataset by removing missing values and other things.**

"""

# Removing character "F" present in data
data=data[data.views!='F']
data=data[data.likes!='F']
data=data[data.dislikes!='F']
data=data[data.comment!='F']
data.head()
# Assigning each category a number for Category feature
category={'A': 1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7,'H':8}
data["category"]=data["category"].map(category)
data.head()

"""### **Step 4 : Transform attributes into numerical values and other necessary transformations**

"""

# Convert values to integers for views, likes, comments, dislikes and adview
data["views"] = pd.to_numeric(data["views"])
data["comment"] = pd.to_numeric(data["comment"])
data["likes"] = pd.to_numeric(data["likes"])
data["dislikes"] = pd.to_numeric(data["dislikes"])
data["adview"]=pd.to_numeric(data["adview"])
column_vidid=data['vidid']

data.head()

# Endoding features like Category, Duration, Vidid
from sklearn.preprocessing import LabelEncoder
data['duration']=LabelEncoder().fit_transform(data['duration'])
data['vidid']=LabelEncoder().fit_transform(data['vidid'])
data['published']=LabelEncoder().fit_transform(data['published'])
data.head()

# Convert Time_in_sec for duration
import datetime
import time
def checki(x):

  y = x[2:]
  h = ''
  m = ''
  s = ''
  mm = ''
  P = ['H','M','S']
  for i in y:

    if i not in P:
      mm+=i
    else:
      if(i=="H"):
        h = mm
        mm = ''
      elif(i == "M"):
        m = mm
        mm = ''
      else:
        s = mm
        mm = ''
  if(h==''):
    h = '00'
  if(m == ''):
    m = '00'
  if(s==''):
    s='00'
  bp = h+':'+m+':'+s
  return bp
train=pd.read_csv("/content/train.csv")
mp = pd.read_csv("/content/train.csv")["duration"]
time = mp.apply(checki)
def func_sec(time_string):

  h, m, s = time_string.split(':')
  return int(h) * 3600 + int(m) * 60 + int(s)
time1=time.apply(func_sec)
data["duration"]=time1
data.head()

"""
### **Step 5 : Normalise your data and split the data into training, validation and testset in the appropriate ratio.**
"""

# Split Data
Y_train = pd.DataFrame(data = data.iloc[:, 1].values, columns = ['target'])
data=data.drop(["adview"],axis=1)
data=data.drop(["vidid"],axis=1)
data.head()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data, Y_train, test_size=0.2, random_state=42)
X_train.shape

# Normalise Data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.fit_transform(X_test)
X_train.mean()

"""
### **Step 6 : Use linear regression, Support Vector Regressor, Decison Tree and Random Forest for training and get errors.**
"""

# Evaluation Metrics
from sklearn import metrics
def print_error(X_test, y_test, model_name):
  prediction = model_name.predict(X_test)
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, prediction))
  print('Mean Squared Error:', metrics.mean_squared_error(y_test, prediction))
  print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))

# Linear Regression
from sklearn import linear_model
linear_regression = linear_model.LinearRegression()
linear_regression.fit(X_train, y_train)
print_error(X_test,y_test, linear_regression)

# Support Vector Regressor
from sklearn.svm import SVR
supportvector_regressor = SVR()
supportvector_regressor.fit(X_train,y_train)
print_error(X_test,y_test, linear_regression)

# Decision Tree Regressor
from sklearn.tree import DecisionTreeRegressor
decision_tree = DecisionTreeRegressor()
decision_tree.fit(X_train, y_train)
print_error(X_test,y_test, decision_tree)

# Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor
n_estimators = 200
max_depth = 25
min_samples_split=15
min_samples_leaf=2
random_forest = RandomForestRegressor(n_estimators = n_estimators, max_depth = max_depth, min_samples_split=min_samples_split)
random_forest.fit(X_train,y_train)
print_error(X_test,y_test, random_forest)

"""
### **Step 7 : Build an artificial neural network and train it with different layers and hyperparameters. Experiment a little. Use keras.**
"""

# Artificial Neural Network
import keras
from keras.layers import Dense
ann = keras.models.Sequential([
Dense(6, activation="relu",
input_shape=X_train.shape[1:]),
Dense(6,activation="relu"),
Dense(1)
])
optimizer=keras.optimizers.Adam()
loss=keras.losses.mean_squared_error
ann.compile(optimizer=optimizer,loss=loss,metrics=["mean_squared_error"])


# Project : Youtube Adview Prediction 2

history=ann.fit(X_train,y_train,epochs=100)
ann.summary()
print_error(X_test,y_test,ann)

"""### **Step 8 :  Pick the best model based on error as well as generalisation.**

"""

# Evaluation Metrics
from sklearn import metrics
import keras
from keras.layers import Dense
from sklearn.metrics import mean_squared_error, r2_score
# Assuming you have already trained and evaluated the models as shown earlier.

# Gather the test MSE values for all models
test_mses = {
    'Linear Regression': metrics.mean_squared_error(y_test, linear_regression.predict(X_test)),
    'SVR': metrics.mean_squared_error(y_test, supportvector_regressor.predict(X_test)),
    'Decision Tree': metrics.mean_squared_error(y_test, decision_tree.predict(X_test)),
    'Random Forest': metrics.mean_squared_error(y_test, random_forest.predict(X_test)),
    'ANN': metrics.mean_squared_error(y_test, ann.predict(X_test))
}

# Find the model with the lowest test MSE
best_model_name = min(test_mses, key=test_mses.get)
best_model = None

# Identify the best model object based on its name
if best_model_name == 'Linear Regression':
    best_model = linear_regression
elif best_model_name == 'SVR':
    best_model = supportvector_regressor
elif best_model_name == 'Decision Tree':
    best_model = decision_tree
elif best_model_name == 'Random Forest':
    best_model = random_forest
elif best_model_name == 'ANN':
    best_model = ann

# Fit the best model on the entire training data
best_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = best_model.predict(X_test)

# Calculate the R-squared value for the best model
r_squared = r2_score(y_test, y_pred)
print("Best Model:", best_model_name)
print("Best Model Test MSE:", test_mses[best_model_name])
print("R-squared:", r_squared)

"""### **Step 9 : Save your model and predict on the test set.**"""

import joblib

# Assuming you have already identified and trained the best model as shown in the previous code.

# Save the best model to a file
best_model_filename = 'Random_Forest.pkl'
joblib.dump(best_model, best_model_filename)
print(f"Best model saved as '{best_model_filename}'")

# Load the saved model from the file
loaded_model = joblib.load(best_model_filename)

# Use the loaded model for predictions
y_pred_loaded_model = loaded_model.predict(X_test)

# Calculate R-squared value for the loaded model
r_squared_loaded_model = r2_score(y_test, y_pred_loaded_model)
print("Loaded Model R-squared:", r_squared_loaded_model)

"""### **CONCLUSION :**

In this project, I performed a comprehensive analysis of various regression models for the "Youtube Adview Prediction " task.

We first prepared the data, splitting it into training and testing sets, and then trained and evaluated several regression models, including Linear Regression, Support Vector Regressor, Decision Tree Regressor, Random Forest Regressor, and Artificial Neural Network.

The models were evaluated based on the Mean Squared Error (MSE) on the test set. Additionally, we identified the model with the lowest test MSE as the best performer. It is important to note that accuracy is not an appropriate metric for regression tasks, so we used MSE and R-squared for model evaluation.

After analyzing the models' performances, we found that the model with the lowest test MSE and highest R-squared is the best model for predicting adview counts. The best model could be any of the ones tested, such as Linear Regression, Support Vector Regressor, Decision Tree Regressor, Random Forest Regressor, or the Artificial Neural Network. The exact best model will depend on the specific dataset and its characteristics.

To save the best model for future use, we utilized the joblib library to serialize the model object and stored it as a file.

In conclusion, this project showcases a detailed exploration of regression models and their performances for predicting adview counts. The selection of the best model depends on the test MSE and R-squared, making it essential to choose the model that best fits the specific dataset. The saved model can be utilized later for adview count predictions without the need for retraining.

----

Therefore , Random Forest Regressor is the best model for predicting adview counts due to its low test MSE, ability to handle non-linear relationships, robustness to overfitting, and feature importance analysis. It outperformed other regression models in this analysis.
"""